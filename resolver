#!/usr/bin/python

import re
import argparse
import json
from os.path import basename, splitext
from collections import defaultdict

from tree2 import Tree, process_string
from algorithm_cls import resolve_anaphor

gold_data = {}
lexicon = {}
head_settings = {}
models = ["deterministic", "neural", "statistical"]

def compute_corenlp_accuracy(args):
    gold_data_file = args.gold_data
    if args.language == 'ger':
        gold_data_file = 'gold_data_ger.json'
    gold_data = read_gold_data(gold_data_file)
    print gold_data
    return


    for model in models:
        print "COMPUTING ACCURACY FOR %s MODEL..." % (model)

        corenlp_data = convert_all_sentences(model)
        eval = [(key,corenlp_data[key]==gold_data[key]) for key in corenlp_data]

        counter = 0
        for i in range(len(eval)):
            if eval[i][1]:
                counter += 1

            elif not eval[i][1]:
                expected = gold_data[eval[i][0]]
                predicted = corenlp_data[eval[i][0]]
                if args.v:
                    print "Sentence number %s does not match." % (eval[i][0])
                    print "\tExpected coreference set: ", expected
                    print "\tOutput coreference set: ", predicted

        accuracy = float(counter)/len(eval)
        print "Accuracy: %s" % (accuracy)


def convert_all_sentences(model):
    with open("dataset/all_files.txt") as f:
        files = ["dataset/"+splitext(basename(x))[0] for x in f]

    dict = defaultdict(tuple)
    for line in files:
        filename = line+"."+model+".json"
        sentNum = basename(line).split("_")[1]
        entry = convert_corenlp_output(filename)
        dict[sentNum] = dict[sentNum] + entry
    return dict

def convert_corenlp_output(filename):
    with open(filename) as f:
        output = json.load(f)
    
    if len(output["corefs"]) > 0:
        for key in output["corefs"]:
            coref_sets = output["corefs"][key]
            sentNum = output["corefs"][key][0]["sentNum"]
            bounds = ()
            for i in range(len(coref_sets)):
                mention = coref_sets[i]
                bounds += (mention["startIndex"], mention["endIndex"])

            return bounds
    else:
        return ()

def read_gold_data(args):
    with open(args) as f:
        gold_data = json.load(f)

    dict = defaultdict(tuple)

    for key in gold_data["corefs"]:
        coref_sets = gold_data["corefs"][key]
        for i in range(len(coref_sets)):
            mention = coref_sets[i]
            dict[mention["sentNum"]] = dict[mention["sentNum"]] + (mention["startIndex"], mention["endIndex"])

    return dict


def main(args):
    global lexicon
    gold_data_file = args.gold_data
    sentences = args.sentences
    if args.language == 'ger':
        gold_data_file = 'gold_data_ger.json'
        sentences = 'auf_deutsch.json'

    with open(args.config_file) as c:
        params = json.load(c)
    lexicon = params[args.language]['lexicon']
    Tree.lexicon = lexicon

    with open(sentences) as s:
        parser_output = json.load(s)

    for sentence in sorted(parser_output["sentences"], key=lambda x: x["index"]):
        list_hier = process_string(sentence["parse"])
        tree_to_parse = Tree(list_hier[0], 0)
        print tree_to_parse.get_string()

        tree_to_parse.pretty_print()

        candidate_matches = {}
        for candidate in filter(lambda x: x.config(), Tree.NP_nodes):
            print "Candidate:", candidate.get_string(), candidate.leaf_range()
            print "Results:"
            candidate_matches[candidate] = {candidate,}
            proposed = resolve_anaphor(candidate, Tree.NP_nodes)
            for proposal in proposed:
                candidate_matches[candidate].add(proposal)
                print proposal.get_string(), proposal.leaf_range()
                

        for grp in candidate_matches.values():
            for grp_1 in candidate_matches.values():
                if grp & grp_1:
                    if len(grp - grp_1) != 0 or len(grp_1-grp) != 0:
                        print "MISMATCH"

        print '-'*20
        Tree.PR_nodes.clear()
        Tree.NP_nodes.clear()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-s",
                        "--sentences",
                        default="binding_dataset.deterministic.json",
                        help="CoreNLP output containing sentences")

    parser.add_argument("-g",
                        "--gold_data",
                        default="gold_data.json",
                        help="Gold data for binding using CoreNLP indexing scheme")

    parser.add_argument("-l",
                       "--language",
                        default="eng",
                        help="Specify 'eng' for English, 'ger' for German")

    parser.add_argument("-c",
                        "--config_file",
                        default="config.json",
                        help="json input for parameter settings. Should not need anything other than the default!")

    parser.add_argument("-v",
                        action="store_true",
                        help="set to true to output mismatched coreference sets")

    parser.add_argument("-o",
                        "--output_file",
                        default="output_tuple.json")

    args = parser.parse_args()
    main(args)
    #compute_corenlp_accuracy(args)
